{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a spark context object\n",
    "appname= \"large_read_tar\"\n",
    "master=\"local\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(appname).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|         col1|            col2|\n",
      "+-------------+----------------+\n",
      "|     abc12edf|[[a,14], [b,10]]|\n",
      "|ddddd12bldasd|[[a,26], [c,20]]|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "df_test = spark.createDataFrame(\n",
    "    [Row(col1=\"abc12edf\", col2=[('a', 14),('b',10)]), \n",
    "     Row(col1=\"ddddd12bldasd\", col2=[('a',26),('c',20)])])\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, '&&&')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,'a'), (1,'b'), (2,'c'), (3,'d'),\n",
    "                      (5,'e'), (8,'&'),(8,'&'),(8,'&')\n",
    "                     ])\n",
    "r = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1),123])\n",
    "r.collect()\n",
    "# reduceByKey and lambda function is your value set\n",
    "a =rdd.reduceByKey(lambda x, y: x+y)\n",
    "a.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 'a')),\n",
       " ('b', (1, 'b')),\n",
       " ('c', (2, 'c')),\n",
       " ('d', (3, 'd')),\n",
       " ('e', (5, 'e')),\n",
       " ('&', (8, '&')),\n",
       " ('&', (8, '&')),\n",
       " ('&', (8, '&'))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_rdd = rdd.keyBy(lambda x: x[1])\n",
    "k_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 'a')),\n",
       " ('c', (2, 'c')),\n",
       " ('e', (5, 'e')),\n",
       " ('b', (1, 'b')),\n",
       " ('d', (3, 'd')),\n",
       " ('&', (24, '&&&'))]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "k_rdd.reduceByKey( lambda x,y: ( x[0]+y[0], x[1] + y[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ('a', 1, 100))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1,100), (\"b\", 3,300), (\"a\", 2,2000)])\n",
    "x_k = x.keyBy(lambda x: x[0])\n",
    "x_k.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change row to new_row--> still return a tuple?\n",
    "def createCombiner(row):\n",
    "    a = len(row)\n",
    "    print(len(row))\n",
    "    return (row[1],row[2], a)\n",
    "\n",
    "\n",
    "# which merges V into C\n",
    "def mergeValue(row, new_row):\n",
    "    return (row[1] + new_row[0], row[2] + new_row[1])\n",
    "    \n",
    "\n",
    "# combine two C's (new row)\n",
    "def mergeCombiners(r1, r2):\n",
    "    return (r1[0]+r2[0], r1[1]+r2[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (3, 2100))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = x_k.combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "r.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [(1, 'a')]),\n",
       " ('c', [(2, 'c')]),\n",
       " ('e', [(5, 'e')]),\n",
       " ('b', [(1, 'b')]),\n",
       " ('d', [(3, 'd')]),\n",
       " ('&', [(8, '&'), (8, '&'), (8, '&')])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 5, 8]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return rez as a tuple ([], count_tweets)\n",
    "def seqOp(rez, row):\n",
    "    \n",
    "    rez[0] + \n",
    "    pass\n",
    "def comOp(p1, p2):\n",
    "    pass\n",
    "\n",
    "r = df_test.rdd.aggregate(([],0), seqOp, comOp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<split(col1, [0-9]+)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Space\n",
    "\n",
    "rez = split(df_test.col1,'[0-9]+')\n",
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       split_op|\n",
      "+---------------+\n",
      "|     [abc, edf]|\n",
      "|[ddddd, bldasd]|\n",
      "+---------------+\n",
      "\n",
      "df_test is not affected \n",
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(rez.alias('split_op')).show()\n",
    "print(\"df_test is not affected \")\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "| 11|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_function(partition):\n",
    "    for row in partition:\n",
    "        row[1] + 'dota2'\n",
    "    \n",
    "t1.foreachPartition(map_function)\n",
    "t1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_test.select('col1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=u'abc12edf')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = a.rdd\n",
    "ar.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(a) --> pyspark.sql.dataframe.DataFrame\n",
    "# when map --> make sure it's in a tuple!!!!!!\n",
    "b = a.rdd.map(lambda x: (x[0].encode('utf-8')+'dota2',))\n",
    "type(b)\n",
    "#b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc12edfdota2',), ('ddddd12bldasddota2',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc12edfdota2_added', 'ddddd12bldasddota2_added']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r= b.map(lambda x: (x[0] + '_added' ))\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all possible types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# class pyspark.sql.types.StructField(name, dataType,\n",
    "# nullable=True, metadata=None)\n",
    "schema =StructType([StructField(\"col2\", StringType(), True),])\n",
    "df_modified = spark.createDataFrame(b, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              col2|\n",
      "+------------------+\n",
      "|     abc12edfdota2|\n",
      "|ddddd12bldasddota2|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
