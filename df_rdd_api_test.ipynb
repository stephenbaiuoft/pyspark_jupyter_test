{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a spark context object\n",
    "appname= \"large_read_tar\"\n",
    "master=\"local\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(appname).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "df_test = spark.createDataFrame(\n",
    "    [Row(col1=\"abc12edf\", col2=11),\n",
    "     Row(col1=\"ddddd12bldasd\", col2=22) ]\n",
    ")\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<split(col1, [0-9]+)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez = split(df_test.col1,'[0-9]+')\n",
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       split_op|\n",
      "+---------------+\n",
      "|     [abc, edf]|\n",
      "|[ddddd, bldasd]|\n",
      "+---------------+\n",
      "\n",
      "df_test is not affected \n",
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(rez.alias('split_op')).show()\n",
    "print(\"df_test is not affected \")\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "| 11|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_function(partition):\n",
    "    for row in partition:\n",
    "        row[1] + 'dota2'\n",
    "    \n",
    "t1.foreachPartition(map_function)\n",
    "t1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_test.select('col1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=u'abc12edf')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = a.rdd\n",
    "ar.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(a) --> pyspark.sql.dataframe.DataFrame\n",
    "# when map --> make sure it's in a tuple!!!!!!\n",
    "b = a.rdd.map(lambda x: (x[0].encode('utf-8')+'dota2',))\n",
    "type(b)\n",
    "#b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc12edfdota2',), ('ddddd12bldasddota2',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc12edfdota2_added', 'ddddd12bldasddota2_added']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r= b.map(lambda x: (x[0] + '_added' ))\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all possible types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# class pyspark.sql.types.StructField(name, dataType,\n",
    "# nullable=True, metadata=None)\n",
    "schema =StructType([StructField(\"col2\", StringType(), True),])\n",
    "df_modified = spark.createDataFrame(b, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              col2|\n",
      "+------------------+\n",
      "|     abc12edfdota2|\n",
      "|ddddd12bldasddota2|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
