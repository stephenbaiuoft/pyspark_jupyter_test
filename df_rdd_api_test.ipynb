{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a spark context object\n",
    "appname= \"large_read_tar\"\n",
    "master=\"local\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(appname).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region to test composite key set\n",
    "rdd = sc.parallelize([(1, '200','aaa'),(2,'100','aaa'),(1,'300','aaa'),\n",
    "                      (2, '2000','bbb'),(1,'3000','bbb'),(2,'1000','ccc')]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '200', 'aaa'),\n",
       " (2, '100', 'aaa'),\n",
       " (1, '300', 'aaa'),\n",
       " (2, '2000', 'bbb'),\n",
       " (1, '3000', 'bbb'),\n",
       " (2, '1000', 'ccc')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, '200'), (1, '200', 'aaa')),\n",
       " ((2, '100'), (2, '100', 'aaa')),\n",
       " ((1, '300'), (1, '300', 'aaa')),\n",
       " ((2, '2000'), (2, '2000', 'bbb')),\n",
       " ((1, '3000'), (1, '3000', 'bbb')),\n",
       " ((2, '1000'), (2, '1000', 'ccc'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr = rdd.keyBy(lambda x: (x[0],x[1]))\n",
    "kr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, '200'), (1, '200', 'aaa')),\n",
       " ((1, '300'), (1, '300', 'aaa')),\n",
       " ((1, '3000'), (1, '3000', 'bbb')),\n",
       " ((2, '100'), (2, '100', 'aaa')),\n",
       " ((2, '1000'), (2, '1000', 'ccc')),\n",
       " ((2, '2000'), (2, '2000', 'bbb'))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.sortByKey(lambda x, y: x[1] if x[1]< y[1] else y[1] ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, '200', 'aaa')),\n",
       " (2, (2, '100', 'aaa')),\n",
       " (1, (1, '300', 'aaa')),\n",
       " (2, (2, '2000', 'bbb')),\n",
       " (1, (1, '3000', 'bbb')),\n",
       " (2, (2, '1000', 'ccc'))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr = rdd.keyBy(lambda x:x[0])\n",
    "kr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, [(2, '100', 'aaa'), (2, '2000', 'bbb'), (2, '1000', 'ccc')]),\n",
       " (1, [(1, '200', 'aaa'), (1, '300', 'aaa'), (1, '3000', 'bbb')])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on reduceByKey:  input and output HAS TO BE OF SAME TYPE!!!!\n",
    "# Implementation of Minimize\n",
    "kr1 = kr.reduceByKey(lambda x, y: x if x[1] < y[1] else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, '100'), (1, '200')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr2 = kr1.mapValues(lambda x: x[1])\n",
    "kr2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ((1, '200', 'aaa'), '200')),\n",
       " (1, ((1, '300', 'aaa'), '200')),\n",
       " (1, ((1, '3000', 'bbb'), '200')),\n",
       " (2, ((2, '100', 'aaa'), '100')),\n",
       " (2, ((2, '2000', 'bbb'), '100')),\n",
       " (2, ((2, '1000', 'ccc'), '100'))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.join(kr2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, '&&&')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,('a','df')), (1,'b'), (2,'c'), (3,'d'),\n",
    "                      (5,'e'), (8,'&'),(8,'&'),(8,'&')\n",
    "                     ])\n",
    "r = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1),123])\n",
    "r.collect()\n",
    "# reduceByKey and lambda function is your value set\n",
    "a =rdd.reduceByKey(lambda x, y: x+y)\n",
    "a.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 'a')),\n",
       " ('b', (1, 'b')),\n",
       " ('c', (2, 'c')),\n",
       " ('d', (3, 'd')),\n",
       " ('e', (5, 'e')),\n",
       " ('&', (8, '&')),\n",
       " ('&', (8, '&')),\n",
       " ('&', (8, '&'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_rdd = rdd.keyBy(lambda x: x[1])\n",
    "k_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ('a', (1, 'a'))),\n",
       " ('b', ('b', (1, 'b'))),\n",
       " ('c', ('c', (2, 'c'))),\n",
       " ('d', ('d', (3, 'd'))),\n",
       " ('e', ('e', (5, 'e'))),\n",
       " ('&', ('&', (8, '&'))),\n",
       " ('&', ('&', (8, '&'))),\n",
       " ('&', ('&', (8, '&')))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 'a')),\n",
       " ('c', (2, 'c')),\n",
       " ('e', (5, 'e')),\n",
       " ('b', (1, 'b')),\n",
       " ('d', (3, 'd')),\n",
       " ('&', (24, '&&&'))]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "k_rdd.reduceByKey( lambda x,y: ( x[0]+y[0], x[1] + y[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ('a', 1, 100))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1,100), (\"b\", 3,300), (\"a\", 2,2000)])\n",
    "x_k = x.keyBy(lambda x: x[0])\n",
    "x_k.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change row to new_row--> still return a tuple?\n",
    "def createCombiner(row):\n",
    "    a = len(row)\n",
    "    print(len(row))\n",
    "    return (row[1],row[2], a)\n",
    "\n",
    "\n",
    "# which merges V into C\n",
    "def mergeValue(row, new_row):\n",
    "    return (row[1] + new_row[0], row[2] + new_row[1])\n",
    "    \n",
    "\n",
    "# combine two C's (new row)\n",
    "def mergeCombiners(r1, r2):\n",
    "    return (r1[0]+r2[0], r1[1]+r2[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (3, 2100))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = x_k.combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "r.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [(1, 'a')]),\n",
       " ('c', [(2, 'c')]),\n",
       " ('e', [(5, 'e')]),\n",
       " ('b', [(1, 'b')]),\n",
       " ('d', [(3, 'd')]),\n",
       " ('&', [(8, '&'), (8, '&'), (8, '&')])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 5, 8]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       split_op|\n",
      "+---------------+\n",
      "|     [abc, edf]|\n",
      "|[ddddd, bldasd]|\n",
      "+---------------+\n",
      "\n",
      "df_test is not affected \n",
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(rez.alias('split_op')).show()\n",
    "print(\"df_test is not affected \")\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "| 11|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_function(partition):\n",
    "    for row in partition:\n",
    "        row[1] + 'dota2'\n",
    "    \n",
    "t1.foreachPartition(map_function)\n",
    "t1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         col1|col2|\n",
      "+-------------+----+\n",
      "|     abc12edf|  11|\n",
      "|ddddd12bldasd|  22|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_test.select('col1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=u'abc12edf')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = a.rdd\n",
    "ar.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(a) --> pyspark.sql.dataframe.DataFrame\n",
    "# when map --> make sure it's in a tuple!!!!!!\n",
    "b = a.rdd.map(lambda x: (x[0].encode('utf-8')+'dota2',))\n",
    "type(b)\n",
    "#b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc12edfdota2',), ('ddddd12bldasddota2',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc12edfdota2_added', 'ddddd12bldasddota2_added']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r= b.map(lambda x: (x[0] + '_added' ))\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all possible types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# class pyspark.sql.types.StructField(name, dataType,\n",
    "# nullable=True, metadata=None)\n",
    "schema =StructType([StructField(\"col2\", StringType(), True),])\n",
    "df_modified = spark.createDataFrame(b, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              col2|\n",
      "+------------------+\n",
      "|     abc12edfdota2|\n",
      "|ddddd12bldasddota2|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
