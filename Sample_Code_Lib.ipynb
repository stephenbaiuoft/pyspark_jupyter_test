{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# NLTK libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a spark context object\n",
    "appname= \"sample_code\"\n",
    "master=\"local\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(appname).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_6(row_v):\n",
    "    rt_dit = row_v[1]\n",
    "    \n",
    "    ak = 'n_a'\n",
    "    a = -1    \n",
    "    bk = 'n_a'\n",
    "    b = -1\n",
    "    ck = 'n_a'\n",
    "    c = -1\n",
    "    dk = 'n_a'\n",
    "    d = -1\n",
    "    ek = 'n_a'\n",
    "    e = -1\n",
    "    fk = 'n_a'\n",
    "    f = -1\n",
    "    \n",
    "    for k, v in rt_dit.iteritems():\n",
    "        av = v[0]/v[1]\n",
    "        if av > a:\n",
    "            # replace count\n",
    "            f = e\n",
    "            e = d\n",
    "            d = c\n",
    "            c = b\n",
    "            b = a\n",
    "            a = av\n",
    "            # replace key          \n",
    "            \n",
    "            fk = ek\n",
    "            ek = dk\n",
    "            dk = ck\n",
    "            ck = bk \n",
    "            bk = ak\n",
    "            ak = k\n",
    "            \n",
    "    return ((ak, a),(bk, b),(ck, c),(dk, d),(ek, e),(fk,f) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do pos and entity extraction for words as well\n",
    "def extract_entity(tokens):\n",
    "    tokens_tag    \n",
    "    # set up for entity extraction \n",
    "    grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    np_ary = []\n",
    "    root_tree = cp.parse(tokens_tag) = nltk.pos_tag(tokens)\n",
    "    for i in list( root_tree.subtrees(filter=lambda x: x.label() == 'NP')):\n",
    "        st = \"\"\n",
    "        for t in i.leaves():\n",
    "            st += t[0] + \" \"\n",
    "        np_ary.append(st)\n",
    "    # entity extracted    \n",
    "    return np_ary\n",
    "    \n",
    "    \n",
    "# return 2 items: sentence_count,  <word_tuple>, <name_entity>\n",
    "def process_tweet(description):\n",
    "    # base case\n",
    "    if description is None or description == \"\":\n",
    "        return (0,[],[])\n",
    "    # filter out http items\n",
    "    description =\\\n",
    "        re.sub(r'https:[^ ]*', '', description, flags=re.IGNORECASE)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(description)    \n",
    "    entity_list = extract_entity(words)\n",
    "    \n",
    "    word_list = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            word_list.append(stemmer.stem(w.lower()))\n",
    "\n",
    "    return (len(sent_tokenize(description)),word_list, entity_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ie_preprocess(\"we saw the yello dog. it is a fat one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].label == 'SS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region to test composite key set\n",
    "rdd = sc.parallelize([(1, '200','aaa'),(2,'100','aaa'),(1,'300','aaa'),\n",
    "                      (2, '2000','bbb'),(1,'3000','bbb'),(2,'1000','ccc')]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, '200', 'aaa')),\n",
       " (2, (2, '100', 'aaa')),\n",
       " (1, (1, '300', 'aaa')),\n",
       " (2, (2, '2000', 'bbb')),\n",
       " (1, (1, '3000', 'bbb')),\n",
       " (2, (2, '1000', 'ccc'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr = rdd.keyBy(lambda x:x[0])\n",
    "kr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region for getting minimum for each key\n",
    "# Region for getting minimum for each key\n",
    "# Region for getting minimum for each key\n",
    "# Region for getting minimum for each key\n",
    "\n",
    "# Note on reduceByKey:  input and output HAS TO BE OF SAME TYPE!!!!\n",
    "# Implementation of Minimize\n",
    "kr1 = kr.reduceByKey(lambda x, y: x if x[1] < y[1] else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, '100'), (1, '200')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr2 = kr1.mapValues(lambda x: x[1])\n",
    "kr2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ((1, '200', 'aaa'), '200')),\n",
       " (1, ((1, '300', 'aaa'), '200')),\n",
       " (1, ((1, '3000', 'bbb'), '200')),\n",
       " (2, ((2, '100', 'aaa'), '100')),\n",
       " (2, ((2, '2000', 'bbb'), '100')),\n",
       " (2, ((2, '1000', 'ccc'), '100'))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr.join(kr2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, '200'), (1, '200', 'aaa')),\n",
       " ((1, '300'), (1, '300', 'aaa')),\n",
       " ((1, '3000'), (1, '3000', 'bbb')),\n",
       " ((2, '100'), (2, '100', 'aaa')),\n",
       " ((2, '1000'), (2, '1000', 'ccc')),\n",
       " ((2, '2000'), (2, '2000', 'bbb'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Region for sorting key pair\n",
    "#Region for sorting key pair\n",
    "#Region for sorting key pair\n",
    "kr.sortByKey(lambda x, y: x[1] if x[1]< y[1] else y[1] ).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
