{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-60a8301539fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mlarge_unzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3a://twitter-data-dump/2013-07/01_10_02.json.bz2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mlarge_unzipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3a://twitter-data-dump/2013-07/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_unzipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#type(data) --> data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/atexit.py\", line 24, in _run_exitfuncs\n",
      "    func(*targs, **kargs)\n",
      "  File \"/usr/local/spark/python/pyspark/shell.py\", line 65, in <lambda>\n",
      "    atexit.register(lambda: sc.stop())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 419, in stop\n",
      "    self._accumulatorServer.shutdown()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in shutdown\n",
      "    SocketServer.TCPServer.shutdown(self)\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 246, in shutdown\n",
      "    self.__is_shut_down.wait()\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 614, in wait\n",
      "    self.__cond.wait(timeout)\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 340, in wait\n",
      "    waiter.acquire()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 236, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 962, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "AttributeError: 'NoneType' object has no attribute 'sc'\n"
     ]
    }
   ],
   "source": [
    "#Imports \n",
    "# This is the run batch for total count for uid \n",
    "# on 40 GB of data and save to a database\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from dateutil.parser import parse\n",
    "# for tokenizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# for schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# for testing \n",
    "import random\n",
    "\n",
    "\n",
    "# instantiate a spark context object\n",
    "appname= \"proto_submit_0\"\n",
    "master_url = \"spark://ec2-52-40-21-59.us-west-2.compute.amazonaws.com:7077\"\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(appname)\\\n",
    "                .master(master_url)\\\n",
    "                .config(\"spark.cassandra.connection.host\", \"ec2-52-40-21-59.us-west-2.compute.amazonaws.com\")\\\n",
    "                .config(\"spark.cassandra.connection.port\", \"9042\")\\\n",
    "                .config(\"spark.eventLog.enabled\", True)\\\n",
    "                .config(\"spark.eventLog.dir\", \"/home/ubuntu/spark_tmp/\")\\\n",
    "                .getOrCreate()\n",
    "\n",
    "# zipped file contains meta data ==> different indice for twitter data dump\n",
    "zip_path = 's3a://twitter-data-dump/test.tar'\n",
    "large_unzipped = 's3a://twitter-data-dump/2013-07/01_10_02.json.bz2'\n",
    "large_unzipped = 's3a://twitter-data-dump/2013-07/'\n",
    "df = spark.read.json(large_unzipped)\n",
    "\n",
    "#type(data) --> data frame\n",
    "\n",
    "# Select interested col attributes\n",
    "main_df = df.selectExpr('id AS tid',\\\n",
    "                        'user.id AS uid',\\\n",
    "                        'text AS tweet',\\\n",
    "                        'user.created_at AS creation_time',\\\n",
    "                        'user.time_zone AS time_zone',\\\n",
    "                        'user.followers_count AS followers_count',\\\n",
    "                        'user.friends_count AS friends_count',\\\n",
    "                        'place.name AS city_name',\\\n",
    "                        'place.country AS country_name',\\\n",
    "                        'entities.media.media_url AS media_ary',\\\n",
    "                        'entities.hashtags.text AS hashtag_ary',\\\n",
    "                        'retweet_count',\\\n",
    "                        'favorite_count'                        \n",
    "                       ).where('tid is NOT NULL AND uid is NOT NULL')\n",
    "\n",
    "#main_df.printSchema()\n",
    "\n",
    "# Main DF column name set\n",
    "col_exp_set = ['tid','uid','tweet','creation_time',\n",
    "               'time_zone','followers_count',\n",
    "               'friends_count','city_name','country_name',\n",
    "               'media_ary','hashtag_ary','retweet_count',\n",
    "               'favorite_count',\n",
    "              ]\n",
    "\n",
    "\n",
    "###### Debuggging 3 records only\n",
    "\n",
    "#base_rdd = main_df.limit(3).rdd\n",
    "#base_rdd.count()\n",
    "base_rdd = main_df.rdd\n",
    "base_rdd.take(1)\n",
    "\n",
    "# return 2 items: sentence_count,  <word_tuple>\n",
    "def process_tweet(description):\n",
    "    # base case\n",
    "    if description is None or description == \"\":\n",
    "        return (0,[])\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_list = []\n",
    " \n",
    "    words = tokenizer.tokenize(description)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            word_list.append(stemmer.stem(w.lower()))\n",
    "\n",
    "    return (len(sent_tokenize(description)),word_list)\n",
    "\n",
    "\n",
    "# parse twitter time string to (date, timestamp_str, hour int)\n",
    "# note later only timestamp_str is changed to asTYpe\n",
    "def parse_time(creation_time):  \n",
    "    # fcn that converts dt to date-str and time-str\n",
    "    def cassandra_convert(dt):\n",
    "        hour = dt.strftime(\"%H\")\n",
    "        # debug:\n",
    "        base_date = datetime.date(2008,4,1)\n",
    "        days_count = random.randint(1,3000)\n",
    "        dt = base_date + timedelta(days=days_count)\n",
    "        return (dt, str(dt),int(hour))       \n",
    "        #return (dt.date(), str(dt),int(hour))\n",
    "    \n",
    "    dt = None\n",
    "    try:\n",
    "        dt = parse(creation_time)\n",
    "    except Exception as e:\n",
    "        # 1. log and \n",
    "        # 2.use current system time instead\n",
    "        dt = datetime.datetime.now()\n",
    "    return cassandra_convert(dt)\n",
    "\n",
    "# Mapping: 1. 13 cols to 15 cols (word_list, date, \n",
    "#          timestamp, hour)\n",
    "#          2. (media_ary-> media_attached; tag_ary -> tag_count)\n",
    "def map_row(row):\n",
    "    # return if image attached\n",
    "    def check_image(media_ary):\n",
    "        if media_ary is None or\\\n",
    "           len(media_ary) == 0:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    # count # of hashtags\n",
    "    def count_tag(tag_ary):\n",
    "        if tag_ary is None:\n",
    "            return 0\n",
    "        return len(tag_ary)\n",
    "    \n",
    "    # create a list of tuples: [(str1, like_num),] or [(str1, ret_num)] \n",
    "    def create_tuple_list(vector, num):\n",
    "        result = [num]\n",
    "        for item in vector:\n",
    "            result.append(item)\n",
    "        if len(result) == 0:\n",
    "            return []\n",
    "        return result\n",
    "        \n",
    "    try:       \n",
    "        tid = row[0]\n",
    "        uid = row[1]\n",
    "        tweet = row[2]\n",
    "        retweet_count = row[11]\n",
    "        favorite_count = row[12]\n",
    "        # Map token: sentence_count, word_list\n",
    "        sentence_count, word_list\\\n",
    "            = process_tweet(tweet)\n",
    "        # Map Time\n",
    "        creation_time = row[3]\n",
    "        date, timestamp, hour\\\n",
    "            = parse_time(creation_time)\n",
    "        time_zone = row[4]\n",
    "        followers_count = row[5]\n",
    "        friends_count = row[6]\n",
    "        city_name = row[7]\n",
    "        country_name = row[8]\n",
    "        media_ary = row[9]\n",
    "        # boolean\n",
    "        media_attached = check_image(media_ary)        \n",
    "        hashtag_ary = row[10]\n",
    "        # count tags\n",
    "        tag_count = count_tag(hashtag_ary)\n",
    "                \n",
    "    except Exception as e:\n",
    "        # 1. log e and 2.return default        \n",
    "        return Row(tid=-1, uid=-1,\n",
    "            followers_count=-1,\n",
    "            friends_count=-1,\n",
    "            tweet='n/a',retweet_count=-1,\n",
    "            favorite_count=-1,sentence_count=-1,\n",
    "            word_list=[],\n",
    "            phrase_list=['n/a'],\n",
    "            date=datetime.date(2001, 1, 1),timestamp='00:00:00',hour=0,\n",
    "            time_zone='n/a',city_name='n/a',\n",
    "            country_name='n/a',\n",
    "            media_attached=False,tag_count=tag_count\n",
    "            )\n",
    "    \n",
    "    r = Row(tid=tid, uid=uid,\n",
    "            followers_count=followers_count,\n",
    "            friends_count=friends_count,\n",
    "            tweet=tweet,retweet_count=retweet_count,\n",
    "            favorite_count=favorite_count,sentence_count=sentence_count,\n",
    "            word_list=word_list,\n",
    "            phrase_list=['n/a'],\n",
    "            date=date,timestamp=timestamp,hour=hour,\n",
    "            time_zone=time_zone,city_name=city_name,\n",
    "            country_name=country_name,\n",
    "            media_attached=media_attached,tag_count=tag_count\n",
    "            )\n",
    "    return r \n",
    "    \n",
    "#base_map_rdd = base_rdd.map(map_row)    \n",
    "\n",
    "# Main DF column name set\n",
    "col_exp_set = ['tid','uid','tweet','followers_count',\n",
    "               'friends_count','retweet_count',\n",
    "               'favorite_count','sentence_count',\n",
    "               'word_list',\n",
    "               'phrase_rt',\n",
    "               'date','timestamp','hour',\n",
    "               'time_zone','city_name','country_name',\n",
    "               'media_attached','tag_count'\n",
    "              ]\n",
    "# Sanity Check\n",
    "base_map_rdd = base_rdd.map(map_row) \n",
    "base_map_rdd.take(1)\n",
    "\n",
    "#len(r) \n",
    "# len(col_exp_set) --> 18\n",
    "\n",
    "# index order\n",
    "#r\n",
    "# city_name=0, country_name=1, date=2, \n",
    "# favorite_count=3, followers_count=4, friends_count=5,\n",
    "# hour=6, media_attached=7, phrase_list=8, \n",
    "# retweet_count=9, sentence_count=10, tag_count=11, tid=12, time_zone=13, \n",
    "# timestamp=14, tweet=15, uid=16, word_list=17\n",
    "\n",
    "# Do total count computing here: map selected ones only\n",
    "# take selected columns from base_map_rdd\n",
    "def map_total_cols(row):\n",
    "    uid_i = 16\n",
    "    tid_i = 12   \n",
    "    rt_c_i = 9\n",
    "    fav_c_i = 3\n",
    "    word_list_i = -1\n",
    "    \n",
    "    return (row[uid_i],row[tid_i],row[rt_c_i],\n",
    "            row[fav_c_i], row[word_list_i])\n",
    "# df_tot    \n",
    "df_tot_raw = base_map_rdd.map(map_total_cols)\n",
    "\n",
    "df_tot_raw_k = df_tot_raw.keyBy(lambda x: x[0])\n",
    "df_tot_raw_k.take(1)\n",
    "#df_tot_raw_k.collect()\n",
    "\n",
    "# helper-function for combineByKey \n",
    "def add_to_dic(dic, word, count):\n",
    "    if word in dic:\n",
    "        k_record = dic.get(word)\n",
    "        dic.update({word: (k_record[0]+count, k_record[1]+1)})\n",
    "    else:\n",
    "        # word and tuple of count and # of times for word\n",
    "        record = {word:(count,1)}\n",
    "        dic.update(record)\n",
    "        \n",
    "def create_dic_set(row):\n",
    "    rt_dic = {}\n",
    "    fav_dic = {}\n",
    "    rt_c = row[2]\n",
    "    fav_c = row[3]\n",
    "\n",
    "    word_list = row[-1]\n",
    "    if rt_c > 0 and fav_c > 0:\n",
    "        for w in word_list:\n",
    "            add_to_dic(rt_dic, w, rt_c)\n",
    "            add_to_dic(fav_dic, w, fav_c)\n",
    "    elif rt_c > 0:\n",
    "        for w in word_list:\n",
    "            add_to_dic(rt_dic, w, rt_c)\n",
    "    elif fav_c > 0 :\n",
    "        for w in word_list:\n",
    "            add_to_dic(fav_dic, w, fav_c)\n",
    "    # return dic_set\n",
    "    return (rt_dic, fav_dic)\n",
    "\n",
    "\n",
    "# uid_i = 0, tid_i = 1, rt_c_i = 2, fav_c_i = 3, word_list_i = -1\n",
    "def createCombiner(row):\n",
    "    #print(\"len of createCombiner row is: \",len(row))\n",
    "    rt_dic, fav_dic = create_dic_set(row)\n",
    "    tid = row[1]\n",
    "    # create a list of tid\n",
    "    return ([tid], rt_dic, fav_dic)\n",
    "\n",
    "# merges 2 dic together\n",
    "def merge_dic(dic1, dic2):\n",
    "    if len(dic1) < len(dic2):\n",
    "        small_dic = dic1\n",
    "        large_dic = dic2\n",
    "    else:\n",
    "        small_dic = dic2\n",
    "        large_dic = dic1\n",
    "    for k in small_dic.keys():\n",
    "        # if k in small_dic\n",
    "        if k in large_dic:\n",
    "            # merge values together\n",
    "            kv1 = small_dic.get(k)\n",
    "            kv2 = large_dic.get(k)\n",
    "            value = (kv1[0]+kv2[0], kv1[1]+kv1[1])\n",
    "            large_dic.update({k:value})\n",
    "        # simply insert to large_dic    \n",
    "        else:\n",
    "            large_dic.update({k:small_dic.get(k)})\n",
    "    # return large_dic        \n",
    "    return large_dic\n",
    "\n",
    "# which merges V into C\n",
    "def mergeValue(new_row, row):\n",
    "    #retrieve last \n",
    "    rt_dic, fav_dic = create_dic_set(row)\n",
    "    tid = row[1]\n",
    "    \n",
    "    rt_dic_merge = merge_dic(rt_dic, new_row[1])\n",
    "    fav_dic_merge = merge_dic(fav_dic, new_row[2])\n",
    "    \n",
    "    tid_list = new_row[0]\n",
    "    tid_list.append(tid)\n",
    "    # return the result\n",
    "    return (tid_list, rt_dic_merge, fav_dic_merge)\n",
    "\n",
    "# combine two C's (new row)\n",
    "def mergeCombiners(r1, r2):\n",
    "    list_merge = r1[0] + r2[0]\n",
    "    rt_dic_merge = merge_dic(r1[1], r2[1])\n",
    "    fav_dic_merge = merge_dic(r1[-1], r2[-1])\n",
    "    \n",
    "    return (list_merge, rt_dic_merge, fav_dic_merge)\n",
    "\n",
    "r = df_tot_raw_k.combineByKey(createCombiner, mergeValue, mergeCombiners)\n",
    "#r.take(1)\n",
    "\n",
    "#r.filter(lambda x: len(x[1][1]) > 0).take(1)\n",
    "\n",
    "def get_top_6(row_v):\n",
    "    rt_dit = row_v[1]\n",
    "    \n",
    "    ak = 'n_a'\n",
    "    a = -1    \n",
    "    bk = 'n_a'\n",
    "    b = -1\n",
    "    ck = 'n_a'\n",
    "    c = -1\n",
    "    dk = 'n_a'\n",
    "    d = -1\n",
    "    ek = 'n_a'\n",
    "    e = -1\n",
    "    fk = 'n_a'\n",
    "    f = -1\n",
    "    \n",
    "    for k, v in rt_dit.iteritems():\n",
    "        av = v[0]/v[1]\n",
    "        if av > a:\n",
    "            # replace count\n",
    "            f = e\n",
    "            e = d\n",
    "            d = c\n",
    "            c = b\n",
    "            b = a\n",
    "            a = av\n",
    "            # replace key          \n",
    "            \n",
    "            fk = ek\n",
    "            ek = dk\n",
    "            dk = ck\n",
    "            ck = bk \n",
    "            bk = ak\n",
    "            ak = k\n",
    "            \n",
    "    return ((ak, a),(bk, b),(ck, c),(dk, d),(ek, e),(fk,f) )\n",
    "                \n",
    "tmp = r.mapValues(get_top_6)\n",
    "\n",
    "tmp.take(1)\n",
    "\n",
    "#tmp.toDF()\n",
    "\n",
    "# build the table schema\n",
    "def build_tuple():\n",
    "    return ArrayType(StructType([\n",
    "        StructField(\"word_name\", StringType(), True),\n",
    "        StructField(\"count\", IntegerType(), False)\n",
    "    ]))\n",
    "\n",
    "def build_schema():\n",
    "    w = build_tuple()\n",
    "    schema = \\\n",
    "    StructType([\n",
    "        StructField(\"uid\",IntegerType(), True),\n",
    "        StructField(\"top_word_list\", ArrayType(StructType([\n",
    "        StructField(\"word_name\", StringType(), True),\n",
    "        StructField(\"count\", IntegerType(), False)])\n",
    "        , True), True)\n",
    "    ])\n",
    "    return schema\n",
    "\n",
    "# fit rdd with schema\n",
    "schema = build_schema()\n",
    "\n",
    "df_uncasted = spark.createDataFrame(tmp, schema)\n",
    "#df_uncasted.printSchema()\n",
    "\n",
    "#df_uncasted.first()\n",
    "\n",
    "def save_data_frame(df, table_name):\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "            mode('append').options(table=table_name,keyspace='twitter').save()    \n",
    "# default to twitter        \n",
    "save_data_frame( df_uncasted, \"batch_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
